{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e42c13f0",
   "metadata": {},
   "source": [
    "Creator - Sarvesh Joshi \n",
    "\n",
    "Date of creation - 14/7/2025\n",
    "\n",
    "Last updated - 14/7/2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73f65076",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62d13779",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node():\n",
    "    def __init__(self,feature = None ,split_value = None,depth = 0,data=None,Gradients = None, Hessians = None):\n",
    "        self.feature = feature\n",
    "        self.split_value = split_value\n",
    "        self.depth = depth\n",
    "        self.data = data\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.Gradients = Gradients\n",
    "        self.Hessians = Hessians \n",
    "        self.is_leaf = True if feature ==-1 else False \n",
    "\n",
    "    def out(self,lambd,alpha):\n",
    "        if self.is_leaf == True:\n",
    "            G = np.sum(self.Gradients)\n",
    "            H = np.sum(self.Hessians)\n",
    "    \n",
    "            if G > alpha:\n",
    "                return -(G-alpha)/(H+lambd)\n",
    "            elif G < -alpha:\n",
    "                return -(G+alpha)/(H+lambd)\n",
    "            else:\n",
    "                return 0\n",
    "    \n",
    "\n",
    "\n",
    "class Tree():\n",
    "    def __init__(self,root,category):\n",
    "        self.root = root\n",
    "        self.category = category\n",
    "        self.leaves = []\n",
    "        self.nodes = []\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc300b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XGBoostMultiClassifier():\n",
    "    def __init__(self,max_depth = 10,loss = \"softmax\",min_split_loss= 0,learning_rate = 1,l2_regularization = 0,max_iter = 1,l1_regularization = 0,min_child_weight=0,subsample = 1):\n",
    "        self.max_depth = max_depth\n",
    "        self.loss = loss.lower()\n",
    "        self.gamma = min_split_loss\n",
    "        self.lambd = l2_regularization\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iter = max_iter\n",
    "        self.alpha = l1_regularization\n",
    "        self.min_child_weight = min_child_weight\n",
    "        self.subsample = subsample\n",
    "        self.rounds = []\n",
    "        self.num_categories = 0\n",
    "    \n",
    "    def label_to_onehot(self,y_train):\n",
    "        if isinstance(y_train,pd.DataFrame) or isinstance(y_train,pd.Series):\n",
    "            y_train=y_train.values\n",
    "        unique_elems, counts = np.unique(y_train, return_counts=True)\n",
    "        onehot_size = len(unique_elems)\n",
    "        y_onehot = np.zeros((y_train.shape[0],onehot_size))\n",
    "        for idx,label in enumerate(y_train):\n",
    "            y_onehot[idx,label] = 1\n",
    "        return y_onehot\n",
    "\n",
    "    def softmax(self,y):\n",
    "        return np.exp(y)/(np.sum(np.exp(y),axis=1,keepdims=True))\n",
    "\n",
    "    \n",
    "    \n",
    "    class Round():\n",
    "        def __init__(self,xgboost_instance,round_num):\n",
    "            self.xgboost = xgboost_instance\n",
    "            self.round_num = round_num \n",
    "            self.trees = []\n",
    "            self.Gradients = None\n",
    "            self.Hessians = None\n",
    "            self.nodes = []\n",
    "            \n",
    "\n",
    "        def compute_gradients(self,y_train,y_guess):\n",
    "            if self.xgboost.loss == \"softmax\":\n",
    "                y_train_onehot = self.xgboost.label_to_onehot(y_train)\n",
    "                y_prob = self.xgboost.softmax(y_guess)\n",
    "                G = y_prob-y_train_onehot\n",
    "                H = (y_prob)*(1-y_prob)\n",
    "\n",
    "            else:\n",
    "                raise ValueError(f\"loss function can only be softmax\")\n",
    "            \n",
    "            self.Gradients = G\n",
    "            self.Hessians = H\n",
    "            #print(self.Hessians)\n",
    "\n",
    "        def find_split_value(self,X,Gradients,Hessians):\n",
    "            feature =-1\n",
    "                # if feature isnt found it stays at -1 and node becomes a leaf\n",
    "            split_value = 0\n",
    "            gamma = self.xgboost.gamma\n",
    "            lambd = self.xgboost.lambd\n",
    "            min_child_weight = self.xgboost.min_child_weight\n",
    "            G = np.sum(Gradients)\n",
    "            H = np.sum(Hessians)\n",
    "            parent_score = 0.5*((G**2)/(H+lambd))\n",
    "            max_gain = -np.inf\n",
    "            \n",
    "\n",
    "            for curr_feature in range(X.shape[1]):  \n",
    "                sort_mask = X[:,curr_feature].argsort()\n",
    "                X_sorted = X[sort_mask]\n",
    "                G_sorted = Gradients[sort_mask]\n",
    "                H_sorted = Hessians[sort_mask]\n",
    "                G_L= 0\n",
    "                H_L = 0\n",
    "                for i in range(len(X_sorted)-1):\n",
    "                    curr_split_value = (X_sorted[i, curr_feature] + X_sorted[i+1, curr_feature]) / 2\n",
    "                    G_L += G_sorted[i]\n",
    "                    H_L += H_sorted[i]\n",
    "                    G_R = G - G_L\n",
    "                    H_R = H - H_L\n",
    "\n",
    "                    # if equal values come then split point will be wrongly detected\n",
    "                    if X_sorted[i,curr_feature] == X_sorted[i+1,curr_feature]:\n",
    "                        continue\n",
    "                    \n",
    "                    curr_gain  = 0.5*((G_L**2)/(H_L + lambd + 1e-12) + (G_R**2)/(H_R+lambd+1e-12)- parent_score) - gamma\n",
    "                    if (curr_gain >= 0) and (curr_gain > max_gain) :\n",
    "                        if (H_L >= min_child_weight) and (H_R>=min_child_weight):\n",
    "                            feature = curr_feature\n",
    "                            split_value = curr_split_value\n",
    "                            max_gain = curr_gain\n",
    "            \n",
    "            return feature,split_value\n",
    "        \n",
    "        def create_node(self,data,depth,Gradients,Hessians):\n",
    "            feature,split_value = self.find_split_value(data,Gradients=Gradients,Hessians=Hessians)\n",
    "\n",
    "            \n",
    "            new_node = Node(feature=feature,split_value=split_value,depth=depth,data=data,Gradients=Gradients,Hessians=Hessians)\n",
    "            self.nodes.append(new_node)\n",
    "            return new_node        \n",
    "        \n",
    "\n",
    "        def build_tree(self,X,num_categories):\n",
    "            \n",
    "            for category in range(num_categories):\n",
    "        \n",
    "                curr_depth = 1\n",
    "                root = self.create_node(data = X,depth=curr_depth,Gradients=self.Gradients[:,category],Hessians=self.Hessians[:,category])\n",
    "                #print(root.is_leaf)\n",
    "                queue = [root]\n",
    "                leaves = []\n",
    "                while((curr_depth+1<=self.xgboost.max_depth) and (len(queue)!=0)):\n",
    "                    curr_node = queue.pop(0)\n",
    "                    curr_depth = curr_node.depth\n",
    "                    curr_data = curr_node.data\n",
    "                    curr_Grads = curr_node.Gradients\n",
    "                    curr_Hess = curr_node.Hessians\n",
    "\n",
    "                    if curr_depth+1 == self.xgboost.max_depth:\n",
    "                        curr_node.is_leaf = True\n",
    "                        leaves.append(curr_node)\n",
    "\n",
    "                    if not curr_node.is_leaf:\n",
    "                        mask_left = curr_data[:,curr_node.feature] < curr_node.split_value\n",
    "                        mask_right = curr_data[:,curr_node.feature] >= curr_node.split_value\n",
    "\n",
    "\n",
    "                        # Empty masks is creating NaN values ahead !\n",
    "                        if (np.sum(mask_left)>0 and np.sum(mask_right) > 0) :\n",
    "\n",
    "                            curr_node.left = self.create_node(depth=curr_depth+1,data = curr_data[mask_left],Gradients= curr_Grads[mask_left],Hessians=curr_Hess[mask_left])\n",
    "                            curr_node.right = self.create_node(depth=curr_depth+1,data = curr_data[mask_right],Gradients= curr_Grads[mask_right],Hessians=curr_Hess[mask_right])\n",
    "                            queue.append(curr_node.left)\n",
    "                            queue.append(curr_node.right)\n",
    "\n",
    "                        else:\n",
    "                            curr_node.is_leaf = True\n",
    "                            leaves.append(curr_node)\n",
    "                    \n",
    "                    elif curr_node.is_leaf:\n",
    "                        leaves.append(curr_node)\n",
    "                \n",
    "                \n",
    "                self.trees.append(Tree(root=root,category=category))\n",
    "                self.trees[-1].leaves = leaves\n",
    "                \n",
    "\n",
    "\n",
    "            # for node in self.nodes:\n",
    "            #     if (node.is_leaf == False) and (node.left == None) and(node.right==None):\n",
    "            #         print(len(node.data))\n",
    "        \n",
    "        def evaluate_tree(self,X,y_guess):\n",
    "            \n",
    "            #print(self.tree.root.left)\n",
    "            \n",
    "            y_guess_new = np.zeros(y_guess.shape)\n",
    "            for i in range(len(X)):\n",
    "                for category in range(y_guess.shape[1]):\n",
    "                    if not self.trees  or self.trees[category].root is None:\n",
    "                        raise ValueError(\"Tree not built yet. Please call fit before predict\")\n",
    "                    curr_node = self.trees[category].root\n",
    "\n",
    "                    \n",
    "                    while curr_node.is_leaf == False:\n",
    "                        curr_feature = curr_node.feature\n",
    "                        curr_split_value = curr_node.split_value\n",
    "\n",
    "                        if X[i,curr_feature] < curr_split_value:\n",
    "                            curr_node = curr_node.left\n",
    "                            \n",
    "                        else :\n",
    "                            curr_node = curr_node.right\n",
    "                        #print(curr_node.is_leaf)\n",
    "                            \n",
    "                    out = (curr_node.out(lambd= self.xgboost.lambd,alpha = self.xgboost.alpha))\n",
    "                    y_guess_new[i,category] = y_guess[i,category] + self.xgboost.learning_rate*out\n",
    "                    #print(curr_node.out(lambd= self.xgboost.lambd,alpha = self.xgboost.alpha))\n",
    "                    #print(f\"Sample {i}, Class {category}, Tree output: {out}\")\n",
    "                \n",
    "            return y_guess_new\n",
    "    \n",
    "\n",
    "    def create_round(self,round_num):\n",
    "        round = self.Round(xgboost_instance=self,round_num=round_num)\n",
    "        return round\n",
    "    \n",
    "    def fit(self,X_train,y_train):\n",
    "        if isinstance(X_train,pd.DataFrame):\n",
    "            X_train = X_train.values\n",
    "        if (isinstance(y_train,pd.DataFrame)) or (isinstance(y_train,pd.Series)):\n",
    "            y_train = y_train.values\n",
    "        y_train = y_train.ravel()\n",
    "        for i in range(self.max_iter):\n",
    "            round_num = i+1\n",
    "            self.rounds.append(self.create_round(round_num))\n",
    "        \n",
    "        \n",
    "        \n",
    "        unique_elems = np.unique(y_train)\n",
    "        num_categories = len(unique_elems)\n",
    "        self.num_categories = num_categories\n",
    "        y_guess = np.zeros((y_train.shape[0],num_categories),dtype = float)\n",
    "        \n",
    "        \n",
    "        for round in self.rounds:\n",
    "            subsample = self.subsample\n",
    "            num_select = int(len(X_train)*subsample)\n",
    "            mask = np.random.choice(len(X_train),size=num_select,replace=False)\n",
    "            X_train_sub = X_train[mask]\n",
    "            y_train_sub = y_train[mask]\n",
    "            y_guess_sub = y_guess[mask]\n",
    "            \n",
    "            round.compute_gradients(y_train_sub,y_guess_sub)\n",
    "            round.build_tree(X_train_sub,self.num_categories)\n",
    "            y_guess = round.evaluate_tree(X_train,y_guess)\n",
    "            \n",
    "        \n",
    "    \n",
    "    def predict_proba(self,X_test):\n",
    "        if isinstance(X_test,pd.DataFrame):\n",
    "            X_test = X_test.values\n",
    "        y_guess = np.zeros((X_test.shape[0],self.num_categories))\n",
    "\n",
    "        for round in self.rounds:\n",
    "            y_guess = round.evaluate_tree(X_test,y_guess)\n",
    "        \n",
    "        y_probs = np.exp(y_guess)/(np.sum(np.exp(y_guess),axis=1,keepdims=True))\n",
    "        #print(y_probs)\n",
    "        #print(\"Raw logits before softmax:\\n\", y_guess)\n",
    "\n",
    "        return y_probs\n",
    "        \n",
    "    def predict(self,X_test):\n",
    "        y_probs = self.predict_proba(X_test)\n",
    "        y_pred = np.argmax(y_probs, axis=1)\n",
    "        return y_pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d0154d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,classification_report\n",
    "\n",
    "\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6d6d8bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        19\n",
      "           1       1.00      1.00      1.00        13\n",
      "           2       1.00      1.00      1.00        13\n",
      "\n",
      "    accuracy                           1.00        45\n",
      "   macro avg       1.00      1.00      1.00        45\n",
      "weighted avg       1.00      1.00      1.00        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "manual_XGB = XGBoostMultiClassifier(subsample=1,max_iter=10,max_depth=4,learning_rate=0.67)\n",
    "manual_XGB.fit(X_train,y_train)\n",
    "y_pred = manual_XGB.predict(X_test)\n",
    "print(classification_report(y_pred,y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80721438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        19\n",
      "           1       1.00      1.00      1.00        13\n",
      "           2       1.00      1.00      1.00        13\n",
      "\n",
      "    accuracy                           1.00        45\n",
      "   macro avg       1.00      1.00      1.00        45\n",
      "weighted avg       1.00      1.00      1.00        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "sklearn_XGB = xgb.XGBClassifier()\n",
    "sklearn_XGB.fit(X_train,y_train)\n",
    "y_pred_inbuilt = sklearn_XGB.predict(X_test)\n",
    "print(classification_report(y_pred_inbuilt,y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

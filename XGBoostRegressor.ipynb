{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0352429",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e3e0f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node():\n",
    "    def __init__(self,feature = None ,split_value = None,depth = 0,data=None,Gradients = None, Hessians = None):\n",
    "        self.feature = feature\n",
    "        self.split_value = split_value\n",
    "        self.depth = depth\n",
    "        self.data = data\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.Gradients = Gradients\n",
    "        self.Hessians = Hessians \n",
    "        self.is_leaf = True if feature ==-1 else False \n",
    "\n",
    "    def out(self,lambd,alpha):\n",
    "        if self.is_leaf == True:\n",
    "            G = np.sum(self.Gradients)\n",
    "            H = np.sum(self.Hessians)\n",
    "    \n",
    "            if G > alpha:\n",
    "                return -(G-alpha)/(H+lambd)\n",
    "            elif G < -alpha:\n",
    "                return -(G+alpha)/(H+lambd)\n",
    "            else:\n",
    "                return 0\n",
    "    \n",
    "\n",
    "\n",
    "class Tree():\n",
    "    def __init__(self,root):\n",
    "        self.root = root"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2a01a1",
   "metadata": {},
   "source": [
    "Basic Structure-\n",
    "\n",
    "For each boosting round t\n",
    "\n",
    "1) Start with a guess y[t] (Ive chosen to start with zeroes, we can also start with mean of y_train)\n",
    "2) Compute the gradients and hessians corresponding to this round for each yi[t] (depends on loss function)\n",
    "3) Build a tree which maximizes the gain at each step based on features and split points.\n",
    "4) For finding the feature and split point corresponding to a node we brute force through all possibilities and choose the one that maximizes gain\n",
    "5) After max_depth is reached in the tree or if the node cannot be split further because gain<0 for all possibilities of feature,split_value then that node becomes a leaf\n",
    "6) Pass x[i],y_guess[i] from the training set through evaluate_tree. Traverse according to the split point and feature condition at each node\n",
    "7) Eventually we reach a leaf from where we output -(np.sum(Gradients)/(np.sum(Hessians)+lambda)) where Gradients and Hessians are the arrays corresponding to the data which will come in this leaf after traversing the tree\n",
    "8) Build max_iter number of rounds and a tree corresponding to each round\n",
    "9) Pass each x[i] from y_test and traverse the entire tree acc to feature and split_value. Once the sample reaches a leaf add the output of that leaf to the current predictions multiplied by the learning_rate \n",
    "10) Do this for all rounds and add all of the outputs up to the initial guess. This is the final prediction\n",
    "\n",
    "Note -- \n",
    "1) It is highly sensitive to learning_rate and initial guess... actual xgboost uses some smoothing or other techniques to decrease sensitivity\n",
    "\n",
    "2) Based on the structure i thought making a class Round which is a subclass of XGBoost would be clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5fa2891",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XGBoostRegressor():\n",
    "    def __init__(self,max_depth = 10,loss = \"mse\",min_split_loss= 0,learning_rate = 1,l2_regularization = 0,max_iter = 1,l1_regularization = 0,min_child_weight=0,subsample = 1):\n",
    "        self.max_depth = max_depth\n",
    "        self.loss = loss.lower()\n",
    "        self.gamma = min_split_loss\n",
    "        self.lambd = l2_regularization\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iter = max_iter\n",
    "        self.alpha = l1_regularization\n",
    "        self.min_child_weight = min_child_weight\n",
    "        self.subsample = subsample\n",
    "        self.rounds = []\n",
    "\n",
    "    \n",
    "    \n",
    "    class Round():\n",
    "        def __init__(self,xgboost_instance,round_num):\n",
    "            self.xgboost = xgboost_instance\n",
    "            self.round_num = round_num \n",
    "            self.tree = None\n",
    "            self.Gradients = None\n",
    "            self.Hessians = None\n",
    "            self.leaves = []\n",
    "            self.nodes = []\n",
    "\n",
    "        def compute_gradients(self,y_train,y_guess):\n",
    "            if self.xgboost.loss == \"mse\":\n",
    "                G = y_guess-y_train\n",
    "                H = np.ones(y_guess.shape)\n",
    "            else:\n",
    "                raise ValueError(f\"loss function can only be mse\")\n",
    "            \n",
    "            self.Gradients = G\n",
    "            self.Hessians = H\n",
    "            #print(self.Hessians)\n",
    "\n",
    "        def find_split_value(self,X,Gradients,Hessians):\n",
    "            feature =-1\n",
    "                # if feature isnt found it stays at -1 and node becomes a leaf\n",
    "            split_value = 0\n",
    "            gamma = self.xgboost.gamma\n",
    "            lambd = self.xgboost.lambd\n",
    "            min_child_weight = self.xgboost.min_child_weight\n",
    "            G = np.sum(Gradients)\n",
    "            H = np.sum(Hessians)\n",
    "            parent_score = 0.5*((G**2)/(H+lambd))\n",
    "            max_gain = -np.inf\n",
    "            \n",
    "\n",
    "            for curr_feature in range(X.shape[1]):  \n",
    "                sort_mask = X[:,curr_feature].argsort()\n",
    "                X_sorted = X[sort_mask]\n",
    "                G_sorted = Gradients[sort_mask]\n",
    "                H_sorted = Hessians[sort_mask]\n",
    "                G_L= 0\n",
    "                H_L = 0\n",
    "                for i in range(len(X_sorted)-1):\n",
    "                    curr_split_value = (X_sorted[i, curr_feature] + X_sorted[i+1, curr_feature]) / 2\n",
    "                    G_L += G_sorted[i]\n",
    "                    H_L += H_sorted[i]\n",
    "                    G_R = G - G_L\n",
    "                    H_R = H - H_L\n",
    "\n",
    "                    # if equal values come then split point will be wrongly detected\n",
    "                    if X_sorted[i,curr_feature] == X_sorted[i+1,curr_feature]:\n",
    "                        continue\n",
    "                    \n",
    "                    curr_gain  = 0.5*((G_L**2)/(H_L + lambd) + (G_R**2)/(H_R+lambd)- parent_score) - gamma\n",
    "                    if (curr_gain >= 0) and (curr_gain > max_gain) :\n",
    "                        if (H_L >= min_child_weight) and (H_R>=min_child_weight):\n",
    "                            feature = curr_feature\n",
    "                            split_value = curr_split_value\n",
    "                            max_gain = curr_gain\n",
    "            \n",
    "            return feature,split_value\n",
    "        \n",
    "        def create_node(self,data,depth,Gradients,Hessians):\n",
    "            feature,split_value = self.find_split_value(data,Gradients=Gradients,Hessians=Hessians)\n",
    "\n",
    "            \n",
    "            new_node = Node(feature=feature,split_value=split_value,depth=depth,data=data,Gradients=Gradients,Hessians=Hessians)\n",
    "            self.nodes.append(new_node)\n",
    "            return new_node        \n",
    "        \n",
    "\n",
    "        def build_tree(self,X):\n",
    "            \n",
    "            \n",
    "            curr_depth = 1\n",
    "            root = self.create_node(data = X,depth=curr_depth,Gradients=self.Gradients,Hessians=self.Hessians)\n",
    "            #print(root.is_leaf)\n",
    "            queue = [root]\n",
    "            leaves = []\n",
    "            while((curr_depth+1<=self.xgboost.max_depth) and (len(queue)!=0)):\n",
    "                curr_node = queue.pop(0)\n",
    "                curr_depth = curr_node.depth\n",
    "                curr_data = curr_node.data\n",
    "                curr_Grads = curr_node.Gradients\n",
    "                curr_Hess = curr_node.Hessians\n",
    "\n",
    "                if curr_depth+1 == self.xgboost.max_depth:\n",
    "                    curr_node.is_leaf = True\n",
    "                    leaves.append(curr_node)\n",
    "\n",
    "                if not curr_node.is_leaf:\n",
    "                    mask_left = curr_data[:,curr_node.feature] < curr_node.split_value\n",
    "                    mask_right = curr_data[:,curr_node.feature] >= curr_node.split_value\n",
    "\n",
    "\n",
    "                    # Empty masks is creating NaN values ahead !\n",
    "                    if (np.sum(mask_left>0) and np.sum(mask_right) > 0) :\n",
    "\n",
    "                        curr_node.left = self.create_node(depth=curr_depth+1,data = curr_data[mask_left],Gradients= curr_Grads[mask_left],Hessians=curr_Hess[mask_left])\n",
    "                        curr_node.right = self.create_node(depth=curr_depth+1,data = curr_data[mask_right],Gradients= curr_Grads[mask_right],Hessians=curr_Hess[mask_right])\n",
    "                        queue.append(curr_node.left)\n",
    "                        queue.append(curr_node.right)\n",
    "\n",
    "                    else:\n",
    "                        curr_node.is_leaf = True\n",
    "                        leaves.append(curr_node)\n",
    "                \n",
    "                elif curr_node.is_leaf:\n",
    "                    leaves.append(curr_node)\n",
    "            \n",
    "            \n",
    "            self.tree = Tree(root=root)\n",
    "            self.leaves = leaves\n",
    "\n",
    "\n",
    "            # for node in self.nodes:\n",
    "            #     if (node.is_leaf == False) and (node.left == None) and(node.right==None):\n",
    "            #         print(len(node.data))\n",
    "        \n",
    "        def evaluate_tree(self,X,y_guess):\n",
    "            if self.tree is None or self.tree.root is None:\n",
    "                raise ValueError(\"Tree not built yet. Please call fit before predict\")\n",
    "            #print(self.tree.root.left)\n",
    "            f = []\n",
    "\n",
    "            for i in range(len(X)):\n",
    "                curr_node = self.tree.root\n",
    "\n",
    "                \n",
    "                while curr_node.is_leaf == False:\n",
    "                    curr_feature = curr_node.feature\n",
    "                    curr_split_value = curr_node.split_value\n",
    "\n",
    "                    if X[i,curr_feature] < curr_split_value:\n",
    "                        curr_node = curr_node.left\n",
    "                        \n",
    "                    else :\n",
    "                        curr_node = curr_node.right\n",
    "                    #print(curr_node.is_leaf)\n",
    "                        \n",
    "                f.append(curr_node.out(lambd= self.xgboost.lambd,alpha = self.xgboost.alpha))\n",
    "                #print(curr_node.out(lambd= self.xgboost.lambd,alpha = self.xgboost.alpha))\n",
    "            f = np.array(f,dtype=float)\n",
    "            f = f.reshape(y_guess.shape)\n",
    "            #print(type(f))\n",
    "            y_guess_new = y_guess + self.xgboost.learning_rate*f\n",
    "            return y_guess_new\n",
    "    \n",
    "\n",
    "    def create_round(self,round_num):\n",
    "        round = self.Round(xgboost_instance=self,round_num=round_num)\n",
    "        return round\n",
    "    \n",
    "    def fit(self,X_train,y_train):\n",
    "        if isinstance(X_train,pd.DataFrame):\n",
    "            X_train = X_train.values\n",
    "        if (isinstance(y_train,pd.DataFrame)) or (isinstance(y_train,pd.Series)):\n",
    "            y_train = y_train.values\n",
    "        y_train = y_train.ravel()\n",
    "        for i in range(self.max_iter):\n",
    "            round_num = i+1\n",
    "            self.rounds.append(self.create_round(round_num))\n",
    "        \n",
    "        \n",
    "        # can use init guess = mean of y_train over here for both training and testing\n",
    "        y_guess = np.zeros(y_train.shape,dtype = float)\n",
    "        \n",
    "        \n",
    "        for round in self.rounds:\n",
    "            subsample = self.subsample\n",
    "            num_select = int(len(X_train)*subsample)\n",
    "            mask = np.random.choice(len(X_train),size=num_select,replace=False)\n",
    "            X_train_sub = X_train[mask]\n",
    "            y_train_sub = y_train[mask]\n",
    "            y_guess_sub = y_guess[mask]\n",
    "            round.compute_gradients(y_train_sub,y_guess_sub)\n",
    "            round.build_tree(X_train_sub)\n",
    "            y_guess = round.evaluate_tree(X_train,y_guess)\n",
    "            \n",
    "        \n",
    "    \n",
    "    def predict(self,X_test):\n",
    "        if isinstance(X_test,pd.DataFrame):\n",
    "            X_test = X_test.values\n",
    "        y_guess = np.zeros(X_test.shape[0])\n",
    "\n",
    "\n",
    "        for round in self.rounds:\n",
    "            y_guess = round.evaluate_tree(X_test,y_guess)\n",
    "        return y_guess \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3c0b6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"advertising.csv\")\n",
    "X = data.drop(columns = [\"Sales\"])\n",
    "y = data[\"Sales\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "743b08c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,train_size=0.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a811d8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0778359999999971"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manual_XGB = XGBoostRegressor(max_depth=200,learning_rate=1,subsample=0.9,min_child_weight=1,l2_regularization=0.0,l1_regularization=0.001,max_iter=20)\n",
    "manual_XGB.fit(X_train,y_train)\n",
    "y_pred = manual_XGB.predict(X_test)\n",
    "mean_squared_error(y_pred=y_pred,y_true=y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a17026a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9973480584548561"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "sklearn_XGB = xgb.XGBRegressor()\n",
    "sklearn_XGB.fit(X_train,y_train)\n",
    "y_pred = sklearn_XGB.predict(X_test)\n",
    "mean_squared_error(y_pred,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb78e25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

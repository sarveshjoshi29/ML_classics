{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0352429",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e3e0f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node():\n",
    "    def __init__(self,feature = None ,split_value = None,depth = 0,data=None,Gradients = None, Hessians = None):\n",
    "        self.feature = feature\n",
    "        self.split_value = split_value\n",
    "        self.depth = depth\n",
    "        self.data = data\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.Gradients = Gradients\n",
    "        self.Hessians = Hessians \n",
    "        self.is_leaf = True if feature ==-1 else False \n",
    "\n",
    "    def out(self,lambd):\n",
    "        if self.is_leaf == True:\n",
    "            G = np.sum(self.Gradients)\n",
    "            H = np.sum(self.Hessians)\n",
    "            return -G/(H+lambd)\n",
    "    \n",
    "\n",
    "\n",
    "class Tree():\n",
    "    def __init__(self,root):\n",
    "        self.root = root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "a5fa2891",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class XGBoost():\n",
    "    def __init__(self,max_depth = 10,loss = \"mse\",min_split_loss= 0,learning_rate = 1,l2_regularization = 0,max_iter = 1):\n",
    "        self.max_depth = max_depth\n",
    "        self.loss = loss.lower()\n",
    "        self.gamma = min_split_loss\n",
    "        self.lambd = l2_regularization\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iter = max_iter\n",
    "        self.rounds = []\n",
    "\n",
    "    \n",
    "    \n",
    "    class Round():\n",
    "        def __init__(self,xgboost_instance,round_num):\n",
    "            self.xgboost = xgboost_instance\n",
    "            self.round_num = round_num \n",
    "            self.tree = None\n",
    "            self.Gradients = None\n",
    "            self.Hessians = None\n",
    "            self.leaves = []\n",
    "            self.nodes = []\n",
    "\n",
    "        def compute_gradients(self,y_train,y_guess):\n",
    "            if self.xgboost.loss == \"mse\":\n",
    "                G = y_guess-y_train\n",
    "                H = np.ones(y_guess.shape)\n",
    "            else:\n",
    "                raise ValueError(f\"loss function can only be mse\")\n",
    "            \n",
    "            self.Gradients = G\n",
    "            self.Hessians = H\n",
    "            #print(self.Hessians)\n",
    "\n",
    "        def find_split_value(self,X,Gradients,Hessians):\n",
    "            feature =-1\n",
    "                # if feature isnt found it stays at -1 and node becomes a leaf\n",
    "            split_value = 0\n",
    "            gamma = self.xgboost.gamma\n",
    "            lambd = self.xgboost.lambd\n",
    "            G = np.sum(Gradients)\n",
    "            H = np.sum(Hessians)\n",
    "            parent_score = 0.5*((G**2)/(H+lambd))\n",
    "            max_gain = -np.inf\n",
    "            \n",
    "\n",
    "            for curr_feature in range(X.shape[1]):  \n",
    "                sort_mask = X[:,curr_feature].argsort()\n",
    "                X_sorted = X[sort_mask]\n",
    "                G_sorted = Gradients[sort_mask]\n",
    "                H_sorted = Hessians[sort_mask]\n",
    "                G_L= 0\n",
    "                H_L = 0\n",
    "                for i in range(len(X_sorted)-1):\n",
    "                    curr_split_value = (X_sorted[i, curr_feature] + X_sorted[i+1, curr_feature]) / 2\n",
    "                    G_L += G_sorted[i]\n",
    "                    H_L += H_sorted[i]\n",
    "                    G_R = G - G_L\n",
    "                    H_R = H - H_L\n",
    "\n",
    "                    if X_sorted[i,curr_feature] == X_sorted[i+1,curr_feature]:\n",
    "                        continue\n",
    "                    \n",
    "                    curr_gain  = 0.5*((G_L**2)/(H_L + lambd) + (G_R**2)/(H_R+lambd)- parent_score) - gamma\n",
    "                    if (curr_gain >= 0) and (curr_gain > max_gain) :\n",
    "                        feature = curr_feature\n",
    "                        split_value = curr_split_value\n",
    "                        max_gain = curr_gain\n",
    "            \n",
    "            return feature,split_value\n",
    "        \n",
    "        def create_node(self,data,depth,Gradients,Hessians):\n",
    "            feature,split_value = self.find_split_value(data,Gradients=Gradients,Hessians=Hessians)\n",
    "\n",
    "            \n",
    "            new_node = Node(feature=feature,split_value=split_value,depth=depth,data=data,Gradients=Gradients,Hessians=Hessians)\n",
    "            self.nodes.append(new_node)\n",
    "            return new_node        \n",
    "        \n",
    "\n",
    "        def build_tree(self,X):\n",
    "            curr_depth = 1\n",
    "            root = self.create_node(data = X,depth=curr_depth,Gradients=self.Gradients,Hessians=self.Hessians)\n",
    "            #print(root.is_leaf)\n",
    "            queue = [root]\n",
    "            leaves = []\n",
    "            while((curr_depth+1<=self.xgboost.max_depth) and (len(queue)!=0)):\n",
    "                curr_node = queue.pop(0)\n",
    "                curr_depth = curr_node.depth\n",
    "                curr_data = curr_node.data\n",
    "                curr_Grads = curr_node.Gradients\n",
    "                curr_Hess = curr_node.Hessians\n",
    "\n",
    "                if curr_depth+1 == self.xgboost.max_depth:\n",
    "                    curr_node.is_leaf = True\n",
    "                    leaves.append(curr_node)\n",
    "\n",
    "                if not curr_node.is_leaf:\n",
    "                    mask_left = curr_data[:,curr_node.feature] < curr_node.split_value\n",
    "                    mask_right = curr_data[:,curr_node.feature] >= curr_node.split_value\n",
    "\n",
    "                    if (np.sum(mask_left>0) and np.sum(mask_right) > 0) :\n",
    "\n",
    "                        curr_node.left = self.create_node(depth=curr_depth+1,data = curr_data[mask_left],Gradients= curr_Grads[mask_left],Hessians=curr_Hess[mask_left])\n",
    "                        curr_node.right = self.create_node(depth=curr_depth+1,data = curr_data[mask_right],Gradients= curr_Grads[mask_right],Hessians=curr_Hess[mask_right])\n",
    "                        queue.append(curr_node.left)\n",
    "                        queue.append(curr_node.right)\n",
    "\n",
    "                    else:\n",
    "                        curr_node.is_leaf = True\n",
    "                        leaves.append(curr_node)\n",
    "                \n",
    "                elif curr_node.is_leaf:\n",
    "                    leaves.append(curr_node)\n",
    "            \n",
    "            \n",
    "            self.tree = Tree(root=root)\n",
    "            self.leaves = leaves\n",
    "\n",
    "\n",
    "            for node in self.nodes:\n",
    "                if (node.is_leaf == False) and (node.left == None) and(node.right==None):\n",
    "                    print(len(node.data))\n",
    "        \n",
    "        def evaluate_tree(self,X,y_guess):\n",
    "            f = []\n",
    "\n",
    "            for i in range(len(X)):\n",
    "                curr_node = self.tree.root\n",
    "\n",
    "                \n",
    "                while curr_node.is_leaf == False:\n",
    "                    curr_feature = curr_node.feature\n",
    "                    curr_split_value = curr_node.split_value\n",
    "\n",
    "                    if X[i,curr_feature] < curr_split_value:\n",
    "                        curr_node = curr_node.left\n",
    "                        \n",
    "                    else :\n",
    "                        curr_node = curr_node.right\n",
    "                    #print(curr_node.is_leaf)\n",
    "                        \n",
    "                f.append(curr_node.out(lambd= self.xgboost.lambd))\n",
    "            f = np.array(f)\n",
    "            f = f.reshape(y_guess.shape)\n",
    "            y_guess_new = y_guess + self.xgboost.learning_rate*f\n",
    "            return y_guess_new\n",
    "    \n",
    "\n",
    "    def create_round(self,round_num):\n",
    "        round = self.Round(xgboost_instance=self,round_num=round_num)\n",
    "        return round\n",
    "    \n",
    "    def fit(self,X_train,y_train):\n",
    "        y_train = y_train.ravel()\n",
    "        for i in range(self.max_iter):\n",
    "            round_num = i+1\n",
    "            self.rounds.append(self.create_round(round_num))\n",
    "        \n",
    "        \n",
    "        # can use init guess = mean of y_train over here for both training and testing\n",
    "        y_guess = np.zeros(y_train.shape,dtype = float)\n",
    "        \n",
    "        \n",
    "        for round in self.rounds:\n",
    "            round.compute_gradients(y_train,y_guess)\n",
    "            round.build_tree(X_train)\n",
    "            y_guess = round.evaluate_tree(X_train,y_guess)\n",
    "            \n",
    "        \n",
    "    \n",
    "    def predict(self,X_test):\n",
    "        \n",
    "        y_guess = np.zeros(X_test.shape[0])\n",
    "\n",
    "\n",
    "        for round in self.rounds:\n",
    "            y_guess = round.evaluate_tree(X_test,y_guess)\n",
    "        return y_guess \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "f3c0b6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"advertising.csv\")\n",
    "X = data.drop(columns = [\"Sales\"]).values\n",
    "y = data[\"Sales\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "743b08c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.553"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,train_size=0.8)\n",
    "manual_XGB = XGBoost(max_depth=200,learning_rate=1)\n",
    "manual_XGB.fit(X_train,y_train)\n",
    "y_pred = manual_XGB.predict(X_test)\n",
    "mean_squared_error(y_pred=y_pred,y_true=y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "a17026a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.9803356541713413"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "sklearn_XGB = HistGradientBoostingRegressor(max_depth=200)\n",
    "sklearn_XGB.fit(X_train,y_train)\n",
    "y_pred = sklearn_XGB.predict(X_test)\n",
    "mean_squared_error(y_pred,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcae6f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19835813",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
